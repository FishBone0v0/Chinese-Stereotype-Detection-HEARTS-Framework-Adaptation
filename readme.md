# Chinese Stereotype Detection: HEARTS Framework Adaptation

## üìã Project Overview

This project adapts the HEARTS framework for stereotype detection from English to Chinese contexts. We construct a culturally-adapted Chinese dataset and fine-tune Chinese pretrained language models (RoBERTa-wwm-ext and MacBERT) to detect stereotypes across multiple social dimensions.

## üéØ Key Features

- **Culturally-Adapted Dataset**: 4,000 Chinese samples across 6 dimensions (Gender, Profession, Nationality, Region, Education, Age)
- **Dual Construction Strategy**: 
  - EMGSD translation for universal dimensions
  - LLM-based data augmentation for Chinese-specific dimensions
- **State-of-the-art Models**: Fine-tuned RoBERTa and MacBERT for Chinese stereotype detection
- **Explainability Analysis**: SHAP and LIME interpretations for model predictions

## üìä Dataset Composition

| Dimension | Target Size | Construction Strategy |
|-----------|-------------|----------------------|
| Gender | 800 | ‚úÖ English ‚Üí Chinese translation from EMGSD |
| Profession | 800 | ‚úÖ English ‚Üí Chinese translation from EMGSD |
| Nationality | 400 | ‚úÖ English ‚Üí Chinese translation from EMGSD |
| Region | 1000 | ‚úÖ LLM-based data augmentation from manual seeds |
| Age | 400 | ‚úÖ LLM-based data augmentation from manual seeds |
| Education | 600 | ‚úÖ LLM-based data augmentation from manual seeds |
| **Total** | **4000** | ‚úÖ Mixed translated + LLM-constructed |

## üóÇÔ∏è Project Structure
```
Chinese_model/
‚îú‚îÄ‚îÄ model_outputs/              # RoBERTa model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-488/
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint-732/
‚îú‚îÄ‚îÄ model_outputs_macbert/      # MacBERT model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-244/
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint-732/
‚îú‚îÄ‚îÄ train_dev_test/             # Train/Dev/Test splits
‚îÇ   ‚îú‚îÄ‚îÄ train.json
‚îÇ   ‚îú‚îÄ‚îÄ dev.json
‚îÇ   ‚îî‚îÄ‚îÄ test.json
‚îú‚îÄ‚îÄ Data/                       # Dataset files
‚îÇ   ‚îú‚îÄ‚îÄ emgsd_selected_en_2000.csv      # Original English EMGSD subset
‚îÇ   ‚îú‚îÄ‚îÄ emgsd_selected_zh_2000.csv      # Chinese translation
‚îÇ   ‚îú‚îÄ‚îÄ final_emgsd_zh.csv              # Final combined dataset (CSV)
‚îÇ   ‚îú‚îÄ‚îÄ final_emgsd_zh.json             # Final combined dataset (JSON)
‚îÇ   ‚îú‚îÄ‚îÄ generated_age.csv               # LLM-generated age stereotypes
‚îÇ   ‚îú‚îÄ‚îÄ generated_education.csv         # LLM-generated education stereotypes
‚îÇ   ‚îú‚îÄ‚îÄ generated_region.csv            # LLM-generated region stereotypes
‚îÇ   ‚îú‚îÄ‚îÄ llm_seeds_zh.json               # Manual seeds for LLM generation
‚îÇ   ‚îú‚îÄ‚îÄ data_create.ipynb               # Dataset construction pipeline
‚îÇ   ‚îî‚îÄ‚îÄ data_process.ipynb              # Data preprocessing scripts
‚îú‚îÄ‚îÄ train_model.ipynb           # Model training notebook
‚îú‚îÄ‚îÄ SHAP-LIME.ipynb            # Explainability analysis
‚îî‚îÄ‚îÄ macbert_shap_lime_bar.png  # Visualization output
```

## üöÄ Quick Start

### 1. Environment Setup

**Install dependencies:**
```bash
pip install -r requirements.txt
```

**Or install manually:**
```bash
pip install torch transformers datasets scikit-learn shap lime matplotlib numpy pandas jupyter
```

**Requirements:**
- Python >= 3.8
- CUDA (optional, for GPU acceleration)

### 2. Dataset Construction

Run the data construction pipeline:
```bash
jupyter notebook Data/data_create.ipynb
```

This will:
- Translate EMGSD samples to Chinese
- Generate Chinese-specific stereotypes using LLM
- Create train/dev/test splits (70%/15%/15%)

### 3. Model Training

Train RoBERTa or MacBERT models:
```bash
jupyter notebook train_model.ipynb
```

**Training Configuration:**
- **Models**: `hfl/chinese-roberta-wwm-ext` or `hfl/chinese-macbert-base`
- **Optimizer**: AdamW (lr=2e-5)
- **Loss Function**: Cross-Entropy Loss
- **Batch Size**: 8
- **Epochs**: 3
- **Max Length**: 128 tokens

### 4. Model Evaluation

The training script automatically evaluates on the test set and provides:
- Overall accuracy and Macro F1-Score
- Dimension-wise performance breakdown
- Classification report

### 5. Explainability Analysis

Run SHAP and LIME interpretations:
```bash
jupyter notebook SHAP-LIME.ipynb
```

This generates visual explanations showing which tokens contribute most to stereotype predictions.

## üìà Results

### Model Performance

| Model | Accuracy | Macro F1-Score |
|-------|----------|----------------|
| RoBERTa (pretrained) | 0.3785 | 0.2746 |
| MacBERT (pretrained) | 0.4064 | 0.3354 |
| RoBERTa (fine-tuned) | 0.7131 | 0.7045 |
| MacBERT (fine-tuned) | **0.7291** | **0.7243** |

### Baseline Comparison

- **Original ALBERT-V2** (English EMGSD): 81.50% Macro F1
- **Our Replication** (ALBERT-V2): 86.45% Macro F1

## üîç Explainability

We provide model interpretations using:
- **SHAP** (SHapley Additive exPlanations): Global feature importance
- **LIME** (Local Interpretable Model-agnostic Explanations): Local instance-level explanations

Example visualization saved in `macbert_shap_lime_bar.png`

## üåç SDG Alignment

This project supports:
- **SDG 5**: Gender Equality
- **SDG 10**: Reduced Inequalities  
- **SDG 16**: Peace, Justice and Strong Institutions

By detecting language-embedded stereotypes in Chinese contexts, this work provides a diagnostic tool for bias monitoring and mitigation.

## ‚ö†Ô∏è Limitations

- **Artificial Bias**: LLM-generated data may introduce artificial biases
- **Generalization Gap**: Benchmark performance doesn't guarantee real-world fairness
- **Misuse Potential**: Automated detection risks being exploited for surveillance without proper governance


## üìß Contact

- **Author**: Qingqing Liu
- **Email**: zczqq26@ucl.ac.uk
- **Institution**: University College London, MSc AI for Sustainable Development
