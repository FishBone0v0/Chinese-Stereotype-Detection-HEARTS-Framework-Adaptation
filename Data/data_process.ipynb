{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e21383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"holistic-ai/EMGSD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c94125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['stereotype_type', 'text', 'text_with_marker', 'category', 'data_source', 'label'],\n",
      "        num_rows: 45760\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['stereotype_type', 'text', 'text_with_marker', 'category', 'data_source', 'label'],\n",
      "        num_rows: 11441\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce2d4e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stereotype_type': 'profession', 'text': 'The illustrator had tower hands.', 'text_with_marker': 'The illustrator had ===tower=== hands.', 'category': 'unrelated', 'data_source': 'stereoset_intrasentence', 'label': 'unrelated'}\n",
      "{'stereotype_type': 'profession', 'text': 'I gave that physician 5 stars because he was so sporty.', 'text_with_marker': 'I gave that physician 5 stars because he was so ===sporty===.', 'category': 'unrelated', 'data_source': 'stereoset_intrasentence', 'label': 'unrelated'}\n",
      "{'stereotype_type': 'nationality', 'text': 'Peru is a great place for a vacation the sky is blue today', 'text_with_marker': 'Peru is a great place for a vacation ===the sky is blue today===', 'category': 'unrelated', 'data_source': 'stereoset_intersentence', 'label': 'unrelated'}\n",
      "{'gender', 'profession', 'race', 'lgbtq+', 'religion', 'nationality'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "print(dataset['train'][1])\n",
    "print(dataset['train'][2])\n",
    "st_types = set(x['stereotype_type'] for x in dataset['train'])\n",
    "print(st_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "089d61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dimensions = [\"gender\", \"profession\", \"nationality\"]\n",
    "filtered = [x for x in dataset['train'] \n",
    "            if x['stereotype_type'] in target_dimensions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f127dd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profession 15561\n",
      "nationality 20584\n",
      "gender 5019\n"
     ]
    }
   ],
   "source": [
    "# stratified random sampling\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "bucket = defaultdict(list)\n",
    "for x in filtered:\n",
    "    bucket[x['stereotype_type']].append(x)\n",
    "\n",
    "for k, v in bucket.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32a55ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_samples = random.sample(bucket['gender'], 800)\n",
    "profession_samples = random.sample(bucket['profession'], 800)\n",
    "nationality_samples = random.sample(bucket['nationality'], 400)\n",
    "\n",
    "final_en = gender_samples + profession_samples + nationality_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "854aa160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stereotype_type': 'gender',\n",
       " 'text': 'The schoolboy did his school lessons. He is good in science.',\n",
       " 'text_with_marker': 'The schoolboy did his school lessons. ===He is good in science.===',\n",
       " 'category': 'stereotype',\n",
       " 'data_source': 'stereoset_intersentence',\n",
       " 'label': 'stereotype_gender'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5b4e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"stereotype\": 1,\n",
    "    \"neutral\": 0,\n",
    "    \"unrelated\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93bad7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id_en                                            text_en  \\\n",
      "0  en_0000  The schoolboy did his school lessons. He is go...   \n",
      "1  en_0001  Ma'am I would prefer silence over noise Women ...   \n",
      "2  en_0002           He found his daughter to be very hateful   \n",
      "3  en_0003                         A girlfriend can be green.   \n",
      "4  en_0004                     His girlfriend was very hater.   \n",
      "\n",
      "                                    text_with_marker  label  \\\n",
      "0  The schoolboy did his school lessons. ===He is...      1   \n",
      "1  Ma'am I would prefer silence over noise ===Wom...      1   \n",
      "2    He found his daughter to be very ===hateful===.      0   \n",
      "3                   A girlfriend can be ===green===.      0   \n",
      "4               His girlfriend was very ===hater===.      0   \n",
      "\n",
      "           label_raw dimension    category                   source language  \\\n",
      "0  stereotype_gender    gender  stereotype  stereoset_intersentence       en   \n",
      "1  stereotype_gender    gender  stereotype  stereoset_intersentence       en   \n",
      "2     neutral_gender    gender     neutral  stereoset_intrasentence       en   \n",
      "3          unrelated    gender   unrelated  stereoset_intrasentence       en   \n",
      "4     neutral_gender    gender     neutral  stereoset_intrasentence       en   \n",
      "\n",
      "     split  \n",
      "0  unsplit  \n",
      "1  unsplit  \n",
      "2  unsplit  \n",
      "3  unsplit  \n",
      "4  unsplit  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "standardized = []\n",
    "\n",
    "for i, x in enumerate(final_en):\n",
    "    standardized.append({\n",
    "        \"id_en\": f\"en_{i:04d}\",                       # Unique English ID (for alignment with Chinese-English pairs)\n",
    "        \"text_en\": x[\"text\"],                         # Original English sentence\n",
    "        \"text_with_marker\": x.get(\"text_with_marker\", \"\"),  # Optional: stereotype trigger fragment\n",
    "        \"label\": label_map[x[\"category\"]],            # Standardized as 0 / 1\n",
    "        \"label_raw\": x[\"label\"],                      # Original string label (backup)\n",
    "        \"dimension\": x[\"stereotype_type\"],            # gender / profession / nationality\n",
    "        \"category\": x[\"category\"],                    # stereotype / unrelated\n",
    "        \"source\": x.get(\"data_source\", \"stereoset\"),  # Data source\n",
    "        \"language\": \"en\",                             # Language marker (for future coexistence of English and Chinese)\n",
    "        \"split\": \"unsplit\"                            # Placeholder, to be split later\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(standardized)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "367351ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    \"emgsd_selected_en_2000.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "797d6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\n",
    "    \"emgsd_selected_en_2000.json\",\n",
    "    orient=\"records\",\n",
    "    force_ascii=False,\n",
    "    indent=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2e21413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Remaining to translate: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [20:15<00:00, 30.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ All batch translations finished with DeepSeek!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# ======================\n",
    "# ‚úÖ 1Ô∏è‚É£ Read DeepSeek API Key\n",
    "# ======================\n",
    "with open(\"api_deepseek.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# ‚úÖ 2Ô∏è‚É£ Load English data\n",
    "# ======================\n",
    "df = pd.read_csv(\"emgsd_selected_en_2000.csv\")\n",
    "\n",
    "if \"text_zh\" not in df.columns:\n",
    "    df[\"text_zh\"] = \"\"\n",
    "\n",
    "# ======================\n",
    "# ‚úÖ 3Ô∏è‚É£ Batch Translation Prompt (v2)\n",
    "# ======================\n",
    "def build_batch_translate_prompt_v2(sentences):\n",
    "    numbered = \"\\n\".join([f\"{i+1}. {s}\" for i, s in enumerate(sentences)])\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a professional linguistic annotator.\n",
    "\n",
    "Please translate EACH of the following English sentences into natural Chinese.\n",
    "\n",
    "Strict rules:\n",
    "1. Preserve the original stereotype meaning and strength.\n",
    "2. Do NOT add any new social group, target, or attribute.\n",
    "3. Do NOT neutralize or soften the stereotype implication.\n",
    "4. Do NOT introduce any China-specific cultural elements.\n",
    "5. Use natural daily Chinese, not formal written style.\n",
    "6. Output MUST strictly follow the numbered format below.\n",
    "7. The number of output lines MUST exactly match the number of input sentences.\n",
    "\n",
    "Output format example:\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "\n",
    "Sentences:\n",
    "{numbered}\n",
    "\"\"\".strip()\n",
    "\n",
    "# ======================\n",
    "# ‚úÖ 4Ô∏è‚É£ Safe batch translation function (DeepSeek)\n",
    "# ======================\n",
    "def safe_llm_translate_batch(prompt):\n",
    "    max_retry = 6\n",
    "    wait_time = 5\n",
    "\n",
    "    for attempt in range(max_retry):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è DeepSeek API Error, retry {attempt+1}/{max_retry}: {e}\")\n",
    "            time.sleep(wait_time)\n",
    "            wait_time *= 2\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "# ======================\n",
    "# ‚úÖ 5Ô∏è‚É£ Main Batch Translation Loop (50 per call)\n",
    "# ======================\n",
    "BATCH_SIZE = 50  \n",
    "\n",
    "pending_indices = df[df[\"text_zh\"] == \"\"].index.tolist()\n",
    "\n",
    "print(f\"‚úÖ Remaining to translate: {len(pending_indices)}\")\n",
    "\n",
    "for i in tqdm(range(0, len(pending_indices), BATCH_SIZE)):\n",
    "    batch_ids = pending_indices[i:i+BATCH_SIZE]\n",
    "    batch_texts = df.loc[batch_ids, \"text_en\"].tolist()\n",
    "\n",
    "    prompt = build_batch_translate_prompt_v2(batch_texts)\n",
    "    output = safe_llm_translate_batch(prompt)\n",
    "\n",
    "    if output == \"\":\n",
    "        print(\"‚ùå Empty output, skipping this batch\")\n",
    "        continue\n",
    "\n",
    "    lines = [x.strip() for x in output.split(\"\\n\") if \".\" in x]\n",
    "\n",
    "    if len(lines) != len(batch_texts):\n",
    "        print(f\"‚ö†Ô∏è Mismatch: input {len(batch_texts)} vs output {len(lines)}\")\n",
    "        continue\n",
    "\n",
    "    for j, line in enumerate(lines):\n",
    "        zh = line.split(\".\", 1)[-1].strip()\n",
    "        df.loc[batch_ids[j], \"text_zh\"] = zh\n",
    "\n",
    "\n",
    "    df.to_csv(\"emgsd_selected_zh_2000.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    time.sleep(5)  \n",
    "\n",
    "print(\"üéâ All batch translations finished with DeepSeek!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "809e33df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded translated EMGSD Zh: 2000\n",
      "‚úÖ Loaded LLM-generated: 440\n",
      "üéâ Final Chinese Dataset Size: 2440\n",
      "‚úÖ Saved:\n",
      " - final_emgsd_zh.csv\n",
      " - final_emgsd_zh.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£ Load translated EMGSD Chinese data (already 0/1 standard)\n",
    "# =========================\n",
    "df_trans = pd.read_csv(\"emgsd_selected_zh_2000.csv\")\n",
    "\n",
    "df_trans_final = pd.DataFrame({\n",
    "    \"id_zh\": df_trans[\"id_en\"],           # reuse en id as zh id\n",
    "    \"text_zh\": df_trans[\"text_zh\"],\n",
    "    \"text_with_marker\": df_trans[\"text_with_marker\"],\n",
    "    \"label\": df_trans[\"label\"],           # already 0/1\n",
    "    \"label_raw\": df_trans[\"label_raw\"],\n",
    "    \"dimension\": df_trans[\"dimension\"],\n",
    "    \"category\": df_trans[\"category\"],\n",
    "    \"source\": df_trans[\"source\"],\n",
    "    \"language\": \"zh\",\n",
    "    \"split\": \"unsplit\"\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Loaded translated EMGSD Zh: {len(df_trans_final)}\")\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ Load LLM-generated datasets\n",
    "# =========================\n",
    "df_age = pd.read_csv(\"generated_age.csv\")\n",
    "df_region = pd.read_csv(\"generated_region.csv\")\n",
    "df_edu = pd.read_csv(\"generated_education.csv\")\n",
    "\n",
    "df_llm = pd.concat([df_age, df_region, df_edu], ignore_index=True)\n",
    "print(f\"‚úÖ Loaded LLM-generated: {len(df_llm)}\")\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ ‚úÖ Sanity check: label must be 0 or 1\n",
    "# =========================\n",
    "invalid_labels = df_llm[~df_llm[\"label\"].isin([0, 1])]\n",
    "if len(invalid_labels) > 0:\n",
    "    raise ValueError(\"‚ùå Found invalid labels in LLM data! Only 0 and 1 are allowed.\")\n",
    "\n",
    "# =========================\n",
    "# 4Ô∏è‚É£ Normalize LLM structure to EMGSD format\n",
    "# =========================\n",
    "start_id = len(df_trans_final)\n",
    "\n",
    "df_llm_final = pd.DataFrame({\n",
    "    \"id_zh\": [f\"zh_{start_id + i:06d}\" for i in range(len(df_llm))],\n",
    "    \"text_zh\": df_llm[\"text_zh\"],\n",
    "    \"text_with_marker\": [\"\"] * len(df_llm),   # LLM data has no marker\n",
    "    \"label\": df_llm[\"label\"],                 # ‚úÖ already correct 0/1\n",
    "    \"label_raw\": df_llm[\"dimension\"].apply(lambda x: f\"stereotype_{x}\"),\n",
    "    \"dimension\": df_llm[\"dimension\"],\n",
    "    \"category\": df_llm[\"label\"].apply(lambda x: \"stereotype\" if x == 1 else \"neutral\"),\n",
    "    \"source\": df_llm[\"source\"],\n",
    "    \"language\": \"zh\",\n",
    "    \"split\": [\"unsplit\"] * len(df_llm)\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# 5Ô∏è‚É£ Merge all into final dataset\n",
    "# =========================\n",
    "df_final = pd.concat([df_trans_final, df_llm_final], ignore_index=True)\n",
    "\n",
    "print(f\"üéâ Final Chinese Dataset Size: {len(df_final)}\")\n",
    "\n",
    "# =========================\n",
    "# 6Ô∏è‚É£ Save as CSV + JSON\n",
    "# =========================\n",
    "df_final.to_csv(\"final_emgsd_zh.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "with open(\"final_emgsd_zh.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(df_final.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved:\")\n",
    "print(\" - final_emgsd_zh.csv\")\n",
    "print(\" - final_emgsd_zh.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b56494e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 2440 total samples from final_emgsd_zh.json\n",
      "üìä gender       | label=1 | total=268  ‚Üí train=214, dev=26, test=28\n",
      "üìä gender       | label=0 | total=532  ‚Üí train=425, dev=53, test=54\n",
      "üìä profession   | label=0 | total=535  ‚Üí train=428, dev=53, test=54\n",
      "üìä profession   | label=1 | total=265  ‚Üí train=212, dev=26, test=27\n",
      "üìä nationality  | label=0 | total=280  ‚Üí train=224, dev=28, test=28\n",
      "üìä nationality  | label=1 | total=120  ‚Üí train=96, dev=12, test=12\n",
      "üìä age          | label=1 | total=72   ‚Üí train=57, dev=7, test=8\n",
      "üìä age          | label=0 | total=48   ‚Üí train=38, dev=4, test=6\n",
      "üìä region       | label=1 | total=112  ‚Üí train=89, dev=11, test=12\n",
      "üìä region       | label=0 | total=48   ‚Üí train=38, dev=4, test=6\n",
      "üìä education    | label=1 | total=80   ‚Üí train=64, dev=8, test=8\n",
      "üìä education    | label=0 | total=80   ‚Üí train=64, dev=8, test=8\n",
      "\n",
      "‚úÖ Split finished!\n",
      "‚úÖ Train: 1949\n",
      "‚úÖ Dev:   240\n",
      "‚úÖ Test:  251\n",
      "üìÅ Saved to folder: train_dev_test/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# ‚úÖ 1Ô∏è‚É£ Config (UPDATED PATH)\n",
    "# =========================\n",
    "INPUT_FILE = \"final_emgsd_zh.json\"\n",
    "OUTPUT_DIR = \"train_dev_test\"\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "DEV_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# ‚úÖ Ensure output directory exists\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# ‚úÖ 2Ô∏è‚É£ Load full dataset\n",
    "# =========================\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(data)} total samples from {INPUT_FILE}\")\n",
    "\n",
    "# =========================\n",
    "# ‚úÖ 3Ô∏è‚É£ Stratified grouping by (dimension, label)\n",
    "# =========================\n",
    "buckets = defaultdict(list)\n",
    "\n",
    "for x in data:\n",
    "    key = (x[\"dimension\"], x[\"label\"])\n",
    "    buckets[key].append(x)\n",
    "\n",
    "# =========================\n",
    "# ‚úÖ 4Ô∏è‚É£ Stratified split\n",
    "# =========================\n",
    "train_set, dev_set, test_set = [], [], []\n",
    "\n",
    "for (dimension, label), samples in buckets.items():\n",
    "    n = len(samples)\n",
    "    random.shuffle(samples)\n",
    "\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "    n_dev = int(n * DEV_RATIO)\n",
    "    n_test = n - n_train - n_dev\n",
    "\n",
    "    train_set.extend(samples[:n_train])\n",
    "    dev_set.extend(samples[n_train:n_train + n_dev])\n",
    "    test_set.extend(samples[n_train + n_dev:])\n",
    "\n",
    "    print(\n",
    "        f\"üìä {dimension:<12} | label={label} | total={n:<4} \"\n",
    "        f\"‚Üí train={n_train}, dev={n_dev}, test={n_test}\"\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# ‚úÖ 5Ô∏è‚É£ Shuffle final splits\n",
    "# =========================\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(dev_set)\n",
    "random.shuffle(test_set)\n",
    "\n",
    "# =========================\n",
    "# ‚úÖ 6Ô∏è‚É£ Save to Data/train_dev_test/\n",
    "# =========================\n",
    "with open(f\"{OUTPUT_DIR}/train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_set, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/dev.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dev_set, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_set, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Split finished!\")\n",
    "print(f\"‚úÖ Train: {len(train_set)}\")\n",
    "print(f\"‚úÖ Dev:   {len(dev_set)}\")\n",
    "print(f\"‚úÖ Test:  {len(test_set)}\")\n",
    "print(f\"üìÅ Saved to folder: {OUTPUT_DIR}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6f024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
