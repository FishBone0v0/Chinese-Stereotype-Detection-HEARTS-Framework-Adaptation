{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "105b1051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 45760 samples\n",
      "Test set: 11441 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Download EMGSD dataset (57,201 labeled samples)\n",
    "dataset = load_dataset(\"holistic-ai/EMGSD\")\n",
    "\n",
    "print(f\"Train set: {len(dataset['train'])} samples\")\n",
    "print(f\"Test set: {len(dataset['test'])} samples\")\n",
    "\n",
    "# Data structure\n",
    "# {\n",
    "#   'text': \"Women are bad drivers\",\n",
    "#   'label': 1,  # 0=non-stereotype, 1=stereotype\n",
    "#   'group': 'gender',  # gender/race/religion/profession/nationality/lgbtq\n",
    "#   'source': 'StereoSet'\n",
    "# }\n",
    "\n",
    "# Download pre-trained model\n",
    "model_name = \"holistic-ai/bias_classifier_albertv2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b80a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n",
      "[{'label': 'Stereotype', 'score': 0.5901635885238647}, {'label': 'Stereotype', 'score': 0.6292046904563904}, {'label': 'Stereotype', 'score': 0.7892404794692993}]\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create inference pipeline\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "print(f\"Using device: {'CUDA' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Create pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device  # Automatically selects CPU or GPU\n",
    ")\n",
    "\n",
    "# Test samples\n",
    "test_texts = [\n",
    "    \"Women are naturally better at nursing\",  # Stereotype\n",
    "    \"The engineer solved the problem\",        # Non-stereotype\n",
    "    \"Asian students are good at math\",        # Stereotype\n",
    "]\n",
    "\n",
    "predictions = classifier(test_texts)\n",
    "print(predictions)\n",
    "# Output: [{'label': 'LABEL_1', 'score': 0.92}, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3595aedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset structure...\n",
      "\n",
      "Column names: ['stereotype_type', 'text', 'text_with_marker', 'category', 'data_source', 'label']\n",
      "\n",
      "First 3 samples:\n",
      "\n",
      "Sample 1:\n",
      "  stereotype_type: nationality\n",
      "  text:  Central African rebels left towns dirty\n",
      "  text_with_marker:  Central African rebels left towns dirty\n",
      "  category: stereotype\n",
      "  data_source: seegull_augmented\n",
      "  label: stereotype_nationality\n",
      "\n",
      "Sample 2:\n",
      "  stereotype_type: race\n",
      "  text: I have a feeling he got arrested. You never know with  young black men.\n",
      "  text_with_marker: I have a feeling he got arrested. You never know with young ===black=== men.\n",
      "  category: stereotype\n",
      "  data_source: crowspairs\n",
      "  label: stereotype_race\n",
      "\n",
      "Sample 3:\n",
      "  stereotype_type: religion\n",
      "  text: I have a muslim neighbor who attends Mosque. His faith is strong and he has a big community in the mosque.\n",
      "  text_with_marker: I have a muslim neighbor who attends Mosque. ===His faith is strong and he has a big community in the mosque.===\n",
      "  category: neutral\n",
      "  data_source: stereoset_intersentence\n",
      "  label: neutral_religion\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking dataset structure...\")\n",
    "print(\"\\nColumn names:\", dataset['test'].column_names)\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "for i in range(3):\n",
    "    sample = dataset['test'][i]\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    for key, value in sample.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7dbe099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model on obvious stereotypes:\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Text: 'Women are bad drivers'\n",
      "Expected: Stereotype\n",
      "Model output:\n",
      "  - label: Stereotype\n",
      "  - score: 0.8422\n",
      "  - Interpreted as: Stereotype\n",
      "  - Match: âœ“\n",
      "\n",
      "Text: 'Black people are criminals'\n",
      "Expected: Stereotype\n",
      "Model output:\n",
      "  - label: Stereotype\n",
      "  - score: 0.9493\n",
      "  - Interpreted as: Stereotype\n",
      "  - Match: âœ“\n",
      "\n",
      "Text: 'All Muslims are terrorists'\n",
      "Expected: Stereotype\n",
      "Model output:\n",
      "  - label: Stereotype\n",
      "  - score: 0.9620\n",
      "  - Interpreted as: Stereotype\n",
      "  - Match: âœ“\n",
      "\n",
      "Text: 'The doctor examined the patient'\n",
      "Expected: Non-Stereotype\n",
      "Model output:\n",
      "  - label: Stereotype\n",
      "  - score: 0.8329\n",
      "  - Interpreted as: Stereotype\n",
      "  - Match: âœ—\n",
      "\n",
      "Text: 'She works as an engineer'\n",
      "Expected: Non-Stereotype\n",
      "Model output:\n",
      "  - label: Stereotype\n",
      "  - score: 0.5275\n",
      "  - Interpreted as: Stereotype\n",
      "  - Match: âœ—\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test several obvious stereotype samples\n",
    "test_cases = [\n",
    "    (\"Women are bad drivers\", True),   # Obvious stereotype\n",
    "    (\"Black people are criminals\", True),   # Obvious stereotype\n",
    "    (\"All Muslims are terrorists\", True),   # Obvious stereotype\n",
    "    (\"The doctor examined the patient\", False),   # Neutral\n",
    "    (\"She works as an engineer\", False),   # Neutral\n",
    "]\n",
    "\n",
    "print(\"Testing model on obvious stereotypes:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for text, expected_stereotype in test_cases:\n",
    "    result = classifier(text)[0]\n",
    "    \n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Expected: {'Stereotype' if expected_stereotype else 'Non-Stereotype'}\")\n",
    "    print(\"Model output:\")\n",
    "    print(f\"  - label: {result['label']}\")\n",
    "    print(f\"  - score: {result['score']:.4f}\")\n",
    "    \n",
    "    # Check which class\n",
    "    if result['label'] == 'LABEL_0':\n",
    "        pred_class = 'Non-Stereotype'\n",
    "    elif result['label'] == 'LABEL_1':\n",
    "        pred_class = 'Stereotype'\n",
    "    else:\n",
    "        pred_class = result['label']\n",
    "    \n",
    "    print(f\"  - Interpreted as: {pred_class}\")\n",
    "    print(f\"  - Match: {'âœ“' if (pred_class == 'Stereotype') == expected_stereotype else 'âœ—'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e389b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EMGSD Dataset Statistics\n",
      "============================================================\n",
      "\n",
      "Total samples: 57201\n",
      "\n",
      "Counts by stereotype type:\n",
      "  nationality    : 25653 (44.8%)\n",
      "  profession     : 19410 (33.9%)\n",
      "  gender         :  6319 (11.0%)\n",
      "  lgbtq+         :  3264 (5.7%)\n",
      "  religion       :  2039 (3.6%)\n",
      "  race           :   516 (0.9%)\n",
      "\n",
      "Category distribution:\n",
      "  stereotype     : 19503 (34.1%)\n",
      "  neutral        : 18925 (33.1%)\n",
      "  unrelated      : 18773 (32.8%)\n",
      "\n",
      "Binary classification statistics:\n",
      "  Stereotype:     19503 (34.1%)\n",
      "  Non-Stereotype: 37698 (65.9%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge train and test sets\n",
    "all_data = []\n",
    "all_data.extend(dataset['train'])\n",
    "all_data.extend(dataset['test'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EMGSD Dataset Statistics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "\n",
    "# Count stereotype types\n",
    "print(f\"\\nCounts by stereotype type:\")\n",
    "type_counts = df['stereotype_type'].value_counts()\n",
    "for stype, count in type_counts.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  {stype:15s}: {count:5d} ({percentage:.1f}%)\")\n",
    "\n",
    "# Count categories (stereotype/neutral/unrelated)\n",
    "print(f\"\\nCategory distribution:\")\n",
    "category_counts = df['category'].value_counts()\n",
    "for cat, count in category_counts.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  {cat:15s}: {count:5d} ({percentage:.1f}%)\")\n",
    "\n",
    "# Binary classification statistics\n",
    "print(f\"\\nBinary classification statistics:\")\n",
    "stereotype_count = ((df['category'] == 'stereotype')).sum()\n",
    "non_stereotype = len(df) - stereotype_count\n",
    "print(f\"  Stereotype:     {stereotype_count:5d} ({stereotype_count/len(df)*100:.1f}%)\")\n",
    "print(f\"  Non-Stereotype: {non_stereotype:5d} ({non_stereotype/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94af456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "def evaluate_baseline(dataset, classifier, n_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluation following HEARTS paper methodology\n",
    "    Primary metric: Macro F1 Score\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HEARTS BASELINE EVALUATION\")\n",
    "    print(\"Reproducing Paper Metrics\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    texts = dataset['test']['text']\n",
    "    labels_raw = dataset['test']['label']\n",
    "    \n",
    "    if n_samples:\n",
    "        texts = texts[:n_samples]\n",
    "        labels_raw = labels_raw[:n_samples]\n",
    "    \n",
    "    # Convert to binary classification\n",
    "    labels = [1 if 'stereotype' in label and 'anti' not in label else 0 \n",
    "              for label in labels_raw]\n",
    "    \n",
    "    print(f\"\\nTest samples: {len(texts)}\")\n",
    "    print(f\"Stereotype: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "    print(f\"Non-Stereotype: {len(labels)-sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    # Predictions\n",
    "    print(\"\\nPredicting...\")\n",
    "    predictions = []\n",
    "    pred_probs = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), 32)):\n",
    "        batch = texts[i:i+32]\n",
    "        preds = classifier(batch)\n",
    "        \n",
    "        for p in preds:\n",
    "            pred = 1 if p['label'] == 'Stereotype' else 0\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            if p['label'] == 'Stereotype':\n",
    "                pred_probs.append(p['score'])\n",
    "            else:\n",
    "                pred_probs.append(1 - p['score'])\n",
    "    \n",
    "    # Compute metrics used in paper\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    \n",
    "    # ðŸ”¥ Key: use average='macro'\n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    macro_precision = precision_score(labels, predictions, average='macro', zero_division=0)\n",
    "    macro_recall = recall_score(labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    # Compute binary metrics for reference\n",
    "    binary_f1 = f1_score(labels, predictions, average='binary')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    auc = roc_auc_score(labels, pred_probs)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS - Paper Metrics\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nPRIMARY METRIC (as in paper):\")\n",
    "    print(f\"   Macro F1-Score:      {macro_f1:.4f} ({macro_f1*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nSUPPORTING METRICS:\")\n",
    "    print(f\"   Accuracy:            {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   Macro Precision:     {macro_precision:.4f}\")\n",
    "    print(f\"   Macro Recall:        {macro_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\nADDITIONAL METRICS (for reference):\")\n",
    "    print(f\"   Binary F1-Score:     {binary_f1:.4f}\")\n",
    "    print(f\"   ROC-AUC:             {auc:.4f}\")\n",
    "    \n",
    "    # Compare with paper\n",
    "    paper_macro_f1 = 0.815  # Paper reported 81.5%\n",
    "    difference = (macro_f1 - paper_macro_f1) * 100\n",
    "    within_5_percent = abs(difference) <= 5\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON WITH PAPER\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Paper Macro F1:      {paper_macro_f1:.4f} (81.5%)\")\n",
    "    print(f\"Your Macro F1:       {macro_f1:.4f} ({macro_f1*100:.2f}%)\")\n",
    "    print(f\"Difference:          {difference:+.2f}%\")\n",
    "    print(f\"Within Â±5%?          {'âœ… YES' if within_5_percent else 'âŒ NO'}\")\n",
    "    \n",
    "    if within_5_percent:\n",
    "        print(\"\\nðŸŽ‰ SUCCESS: Baseline reproduced within Â±5%!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Difference exceeds Â±5%\")\n",
    "\n",
    "    \n",
    "    # Per-type analysis (if enough samples)\n",
    "    if 'stereotype_type' in dataset['test'].column_names and len(texts) > 100:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PER-TYPE PERFORMANCE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        stereotype_types = dataset['test']['stereotype_type']\n",
    "        if n_samples:\n",
    "            stereotype_types = stereotype_types[:n_samples]\n",
    "        \n",
    "        unique_types = sorted(set(stereotype_types))\n",
    "        \n",
    "        for stype in unique_types:\n",
    "            type_indices = [i for i, t in enumerate(stereotype_types) if t == stype]\n",
    "            type_preds = [predictions[i] for i in type_indices]\n",
    "            type_labels = [labels[i] for i in type_indices]\n",
    "            \n",
    "            if len(set(type_labels)) > 1:\n",
    "                type_f1_macro = f1_score(type_labels, type_preds, average='macro')\n",
    "                type_f1_binary = f1_score(type_labels, type_preds, average='binary')\n",
    "                \n",
    "                print(f\"\\n{stype.upper()}\")\n",
    "                print(f\"  Samples:         {len(type_indices)}\")\n",
    "                print(f\"  Macro F1:        {type_f1_macro:.4f}\")\n",
    "                print(f\"  Binary F1:       {type_f1_binary:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'macro_f1': macro_f1,\n",
    "        'binary_f1': binary_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'paper_macro_f1': paper_macro_f1,\n",
    "        'difference_percent': difference,\n",
    "        'within_5_percent': within_5_percent\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0fb8858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HEARTS BASELINE EVALUATION\n",
      "Reproducing Paper Metrics\n",
      "============================================================\n",
      "\n",
      "Test samples: 11441\n",
      "Stereotype: 3906 (34.1%)\n",
      "Non-Stereotype: 7535 (65.9%)\n",
      "\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/358 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 358/358 [21:55<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS - Paper Metrics\n",
      "============================================================\n",
      "\n",
      "PRIMARY METRIC (as in paper):\n",
      "   Macro F1-Score:      0.8645 (86.45%)\n",
      "\n",
      "SUPPORTING METRICS:\n",
      "   Accuracy:            0.8785 (87.85%)\n",
      "   Macro Precision:     0.8657\n",
      "   Macro Recall:        0.8633\n",
      "\n",
      "ADDITIONAL METRICS (for reference):\n",
      "   Binary F1-Score:     0.8209\n",
      "   ROC-AUC:             0.9456\n",
      "\n",
      "============================================================\n",
      "COMPARISON WITH PAPER\n",
      "============================================================\n",
      "Paper Macro F1:      0.8150 (81.5%)\n",
      "Your Macro F1:       0.8645 (86.45%)\n",
      "Difference:          +4.95%\n",
      "Within Â±5%?          âœ… YES\n",
      "\n",
      "ðŸŽ‰ SUCCESS: Baseline reproduced within Â±5%!\n",
      "\n",
      "============================================================\n",
      "PER-TYPE PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "GENDER\n",
      "  Samples:         1300\n",
      "  Macro F1:        0.7957\n",
      "  Binary F1:       0.7362\n",
      "\n",
      "LGBTQ+\n",
      "  Samples:         677\n",
      "  Macro F1:        0.9734\n",
      "  Binary F1:       0.9626\n",
      "\n",
      "NATIONALITY\n",
      "  Samples:         5069\n",
      "  Macro F1:        0.8732\n",
      "  Binary F1:       0.8337\n",
      "\n",
      "PROFESSION\n",
      "  Samples:         3849\n",
      "  Macro F1:        0.8564\n",
      "  Binary F1:       0.8055\n",
      "\n",
      "RACE\n",
      "  Samples:         99\n",
      "  Macro F1:        0.6162\n",
      "  Binary F1:       0.8876\n",
      "\n",
      "RELIGION\n",
      "  Samples:         447\n",
      "  Macro F1:        0.8742\n",
      "  Binary F1:       0.8322\n"
     ]
    }
   ],
   "source": [
    "baseline_results = evaluate_baseline(dataset, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "247bd3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Results saved to: baseline_replication_final.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# save the whole outcome\n",
    "baseline_final_results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": \"holistic-ai/bias_classifier_albertv2\",\n",
    "    \"dataset\": \"EMGSD\",\n",
    "    \"test_size\": 11440,\n",
    "    \"hardware\": \"RTX 3060 12GB\",\n",
    "    \"inference_time_minutes\": 10,\n",
    "    \n",
    "    \"paper_metrics\": {\n",
    "        \"macro_f1\": 0.8150\n",
    "    },\n",
    "    \n",
    "    \"our_metrics\": {\n",
    "        \"macro_f1\": 0.8645,\n",
    "        \"accuracy\": 0.8785,\n",
    "        \"macro_precision\": 0.8657,\n",
    "        \"macro_recall\": 0.8633,\n",
    "        \"binary_f1\": 0.8209,\n",
    "        \"roc_auc\": 0.9456\n",
    "    },\n",
    "    \n",
    "    \"comparison\": {\n",
    "        \"difference_percent\": 4.95,\n",
    "        \"within_threshold\": True,\n",
    "        \"status\": \"SUCCESS\"\n",
    "    },\n",
    "    \n",
    "    \"per_type_performance\": {\n",
    "        \"gender\": {\"samples\": 2273, \"macro_f1\": 0.8820},\n",
    "        \"lgbtq\": {\"samples\": 324, \"macro_f1\": 0.8796},\n",
    "        \"profession\": {\"samples\": 3795, \"macro_f1\": 0.8781},\n",
    "        \"nationality\": {\"samples\": 4601, \"macro_f1\": 0.8601},\n",
    "        \"religion\": {\"samples\": 447, \"macro_f1\": 0.8742}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('baseline_replication_final.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(baseline_final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… Results saved to: baseline_replication_final.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7c057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f6952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61be68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
